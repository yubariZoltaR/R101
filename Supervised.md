##  Supervised Learning(ì§€ë„í•™ìŠµ)

Standardization : (data - avg) / s.d
MinMax Scale = (data - min(data)) / (max(data) - min(data)) : 0~1 ì‚¬ì´ë¡œ ë°ì´í„°ë¥¼ ìœ„ì¹˜ì‹œí‚´

ëª¨í˜•í‰ê°€ê°œë… 
![image](https://user-images.githubusercontent.com/80673078/111432382-4cbc5600-8740-11eb-96b6-5dfbe666e963.png)

ROC curve (xì¶•ì— false positive rate, yì¶•ì— ë¯¼ê°ë„) : ê³¡ì„  ì•„ë˜ì˜ ë©´ì (AUC)ê°€ ë„“ì„ìˆ˜ë¡ ì¢‹ì€ ëª¨ë¸ì´ë©° 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì¢‹ë‹¤. 
MSE(Mean Squared Error) 

Overfitting, Underfitting

Cross-Validation : ë°ì´í„°ë¥¼ trainìš©ê³¼ testìš©ìœ¼ë¡œ ë¶„ë¦¬í•œ ì´í›„ ì •í™•ë„ ì¸¡ì •ì— test ë°ì´í„° ì‚¬ìš© (ì¢…ë¥˜ëŠ” ë‹¤ì–‘)


#### 1\. k-nearest neighbor
ê°€ì¥ ê°€ê¹Œì´ ìˆëŠ” ë°ì´í„°ì— ì†í•œë‹¤ê³  ë³´ëŠ” ë°©ë²•

kappaí†µê³„ëŸ‰ : (ê´€ì¸¡ëœ ì •í™•ë„ - ê¸°ëŒ€ ì •í™•ë„) / (1 - ê¸°ëŒ€ ì •í™•ë„) -1ê³¼ 1ì‚¬ì´ì´ë©° 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì¢‹ë‹¤.

``` r
install.packages("caret", dependencies = TRUE)
library(caret)
rawdata <- read.csv("data.csv", header= TRUE)
rawdata$Class <- as.factor(rawdata$Class)
str(rawdata)

analdata <- rawdata
set.seed(2021)                        # ê°™ì€ ê²°ê³¼ ë„ì¶œí•˜ë„ë¡ íŠ¹ì • ìˆ«ì ì§€ì •. 
datatotal <- sort(sample(nrow(analdata), nrow(analdata)*0.7)) 
# 7:3ìœ¼ë¡œ train, test ë°ì´í„° ë¶„ë¦¬, sortëŠ” ê²°ê³¼ í¸ì˜ë¥¼ ìœ„í•œ ì˜¤ë¦„ì°¨ìˆœ ì •ë¦¬, nrowëŠ” ë°ì´í„°í–‰ìˆ˜, sample(a,b)ëŠ” 1ë¶€í„° aê¹Œì§€ ìˆ«ìì¤‘ bì¶”ì¶œ
train <- rawdata[datatotal,]
test <- rawdata[-datatotal,]
train_x <- train[,1:13]
train_y <- train[,14]
test_x <- test[,1:13]
test_y <- test[,14]

ctrl <- trainControl(
          method = "repeatedcv",      # cross-validation ë°˜ë³µ
          number = 10,                # í›ˆë ¨ ë°ì´í„° fold ê°œìˆ˜
          repeats = 5                 # cvë°˜ë³µ íšŸìˆ˜
) 
customGrid <- expand.grid(k=1:10)     # ë²¡í„°, ì¸ì ì¡°í•©ì¸ ë°ì´í„°í”„ë ˆì„ì„ ìƒì„±
knnFit <- train(Class~.,              # íƒ€ê²Ÿ / ë¬¼ê²°í‘œì‹œ ë‹¤ìŒ ì ì€ í”¼ì³ë¥¼ ëª¨ë‘ 
                data = train,
                method = "knn",                     # ì‚¬ìš©í•  ë¨¸ì‹ ëŸ¬ë‹ ë°©ë²•
                trControl = trainControl(),         # í•™ìŠµ ë°©ë²•
                preProcess = c("center", "scale"),  # í‘œì¤€í™”
                tuneGrid = expand.grid(k=1:10)      # íŠœë‹ íŒŒë¼ë¯¸í„° ê°’ ëª©ë¡
                metric = "Accuracy")                # ëª¨í˜• í‰ê°€ ë°©ì‹
knnFit
plot(knnFit)

pred_test <- predict(knnFit, newdata=test) #testë°ì´í„°ë¥¼ ì˜ ì˜ˆì¸¡í•˜ëŠ”ì§€
confusionMatrix(pred_test, test$Class)

importance_knn <- varImp(knnFit, scale = FALSE) #ë³€ìˆ˜ ì¤‘ìš”ë„
plot(importance_knn)
``` 

#### 2\. Logistic Regression
ì„ í˜•íšŒê·€ë¶„ì„ì´ ì¢…ì†ë³€ìˆ˜ê°€ ì—°ì†í˜•ë§Œ ê°€ëŠ¥í•˜ë©° ë¬´ì œí•œë²”ìœ„ë¥¼ ê°–ì§€ë§Œ, ë¡œì§€ìŠ¤í‹± íšŒê·€ë¶„ì„ì€ ë²”ì£¼í˜•, ì—°ì†í˜• ëª¨ë‘ ê°€ëŠ¥í•˜ë‚˜ ë²”ìœ„ëŠ” ì œí•œì´ ìˆë‹¤.   
z=Î±+ğ›½ğ‘¥ ì—ì„œ zì— ì œí•œì´ ìˆë„ë¡ í•˜ê¸° ìœ„í•´ì„œ logâ¡(ğ‘¦/(1âˆ’ğ‘¦))=ğ›¼+ğ›½ğ‘¥ í˜•íƒœë¡œ ë³€í˜•.   
ë¡œì§€ìŠ¤í‹± íšŒê·€ë¶„ì„ train methodì—ëŠ”    
Boosted Logistic Regression : method = "LogitBoost" ê°€ì¥ ê°„ë‹¨í•œ ëª¨í˜•ì—ì„œ ì‹œì‘í•´ ì—¬ëŸ¬ í”¼ì³ë¥¼ ë”í•˜ëŠ” ë°©ì‹   
Logistic Model Trees : method = "LMT" ì˜ì‚¬ê²°ì •ë‚˜ë¬´ì™€ ê²°í•©ëœ ëª¨í˜•   
Penalized Logistic Regression : method = "plr" ì •ê·œí™”ë¥¼ ë‘ì–´ì„œ ëª¨ë¸ì˜ ë³µì¡ì„±ì„ ì¡°ì ˆí•¨ìœ¼ë¡œì¨ ì˜¤ë²„í”¼íŒ… íšŒí”¼ (ğœ†ì˜ í¬ê¸°ì— ë”°ë¼ ë² íƒ€ê°’ì˜ ì˜ì—­ ë‹¬ë¼ì§)   
Regularized Logistic Regression : method = "regLogistic"    

``` r
rawdata2 <- read.csv("data2.csv", header= TRUE)
str(rawdata2)
rawdata2$target <- as.factor(rawdata2$target)
# ì´í•˜ ì—°ì†í˜• ë…ë¦½ë³€ìˆ˜ í‘œì¤€í™”
rawdata$age <- scale(rawdata$age)
rawdata$trestbps <- scale(rawdata$trestbps)
rawdata$chol <- scale(rawdata$chol)
rawdata$thalach <- scale(rawdata$thalach)
rawdata$oldpeak <- scale(rawdata$oldpeak)
rawdata$slope <- scale(rawdata$slope)
# ì´í•˜ ë²”ì£¼í˜• ë…ë¦½ë³€ìˆ˜ í‘œì¤€í™”
newdata <- rawdata
factorVar <- c("sex", "cp", "fbs", "restecg", "exang", "ca", "thal")
newdata[ ,factorVar] = lapply(newdata[ ,factorVar], factor)

set.seed(2021)                        
datatotal <- sort(sample(nrow(newdata), nrow(newdata)*0.7)) 
train <- rawdata[datatotal,]
test <- rawdata[-datatotal,]
train_x <- train[,1:13]
train_y <- train[,14]
test_x <- test[,1:13]
test_y <- test[,14]

ctrl <- trainControl(method = "repeatedcv", repeats=5)
logitFit <- train(target~.,
                  data=train,
                  method = "LogitBoost",
                  trControl = ctrl,
                  metric = "Accuracy")
logitFit
plot(logitFit)

pred_test <- predict(logitFit, newdata=test)
confusionMatrix(pred_test, test$target)

importance_logit <- varImp(logitFit, scale =FALSE)
plot(importance_logit)
```

#### 3\. Naive Bayes Classification
Bayes Theorem í™œìš© ğ‘ƒ(ğ‘‹â”‚ğ‘Œ)=(ğ‘ƒ(ğ‘Œâ”‚ğ‘‹)ğ‘ƒ(ğ‘‹))/(ğ‘ƒ(ğ‘Œ)) -> ìì„¸í•œê±´ pdfì°¸ì¡°(p.30)     
ì•ì—ì„œì™€ ë§ˆì°¬ê°€ì§€ë¡œ methodì—ëŠ” ë‹¨ìˆœí•œ "naive_bayes" ì™¸ì—ë„ "nbDiscrete", "manb", "awnb", "nb"ê°€ì¡´ì¬     
ê²°ê³¼ì— ë‚˜ì˜¤ëŠ” usekernel : ì»¤ë„ë°€ë„ì¶”ì •ìœ¼ë¡œ smoothing / adjust : bandwidthì¡°ì •ìœ¼ë¡œ ì¶”ì •ì»¤ë„ë°€ë„í•¨ìˆ˜ ëª¨ì–‘ë³€í™” / laplace : ë¼í”Œë¼ìŠ¤ ìŠ¤ë¬´ë”©.ì ì€ ë°ì´í„°ì—ì„œ ê·¹ë‹¨ì ì¸ ê°’ ì¶”ì • ë°©ì§€      
ê²°ê³¼ì°½ ë§ˆì§€ë§‰ì˜ adjustëŠ” usekernel = Trueì¼ë•Œ ìœ ì˜ë¯¸í•œ ê°’ì´ë‹¤.     
ì•ì—ì„œê¹Œì§€ train,testêµ¬ë¶„ê¹Œì§€ ëª¨ë“  ê²ƒì€ ë™ì¼í•˜ë‹¤. ì´í›„ predictì´ë‚˜ ë³€ìˆ˜ì¤‘ìš”ë„ ê³¼ì • ì—­ì‹œ ë™ì¼ë¡œ ìƒëµí•œë‹¤.   
``` r
ctrl <- trainControl(method = "repeatedcv", repeats = 5)
nbFit <- train(Class~.,
               data=train,
               method = "naive_bayes",
               trControl = ctrl,
               preProcess = c("center", "scale"),
               metric="Accuracy")
nbFit
plot(nbFit)
``` 

#### 4\. Decision Tree & Random Forest
ì´ë¡  ìì²´ëŠ” pdfì°¸ì¡°. ì—¬ê¸°ì„œ ì„¤ëª…í•˜ê¸°ì—” ë³µì¡.      
Decision Treeì˜ ë‹¨ì ì¸ Overfitting ë³´ì™„í•˜ê¸°ìœ„í•´ Random Forest í™œìš©   
Random Forest : nê°œì˜ ëœë¤í•œ ë°ì´í„° ìƒ˜í”Œ ì¤‘ë³µí•´ì„œ ì¶”ì¶œ, dê°œì˜ featureì„ ì¤‘ë³µì—†ì´ ì¶”ì¶œí•˜ì—¬ Decision Treeí•™ìŠµí•˜ê³  ê²°ê³¼í• ë‹¹ ->for detail pdf   

train,testê¹Œì§€ëŠ” ìœ„ì™€ ë™ì¼í•œ ê³¼ì •ìœ¼ë¡œ ìˆ˜í–‰í•¨.  
``` r
#Decision Tree

install.packages("tree")
library(tree)
treeRaw <- tree(Class~., data=train)
plot(treeRaw)
text(treeRaw) # ì—¬ê¸°ê¹Œì§€ í•˜ë©´ ê·¸ë¦¼ì´ ê·¸ë ¤ì§ˆ ê²ƒ

cv_tree <- cv.tree(treeRaw, FUN=prune.misclass) # FUN : ê°€ì§€ì¹˜ê¸° í•¨ìˆ˜ ì„ íƒ (prune.misclassëŠ” ì˜¤ë¶„ë¥˜ê¸°ì¤€)
plot(cv_tree)
prune_tree <- prune.misclass(treeRaw, best=4) #best=4ëŠ” cvë¥¼ í†µí•´ êµ¬í•œ ì‚¬ì´ì¦ˆ
plot(prune_tree)
text(prune_tree, pretty=0) #pretty=0ì€ ë¶„í• í”¼ì³ ì´ë¦„ì„ ë°”ê¾¸ì§€ ì•Šê² ë‹¤ëŠ” ê²ƒ

pred <- predict(prune_tree, test, typle='class')
confusionMatrix(pred,test$Class)

#Random Forest
ctrl <- trainControl(method="repeatedcv",repeats = 5)  
rfFit <- train(Class ~ ., 
               data = train, 
               method = "rf", 
               trControl = ctrl, 
               preProcess = c("center","scale"),
               metric="Accuracy")                     # ê²°ê³¼ ì°½ì˜ mtry ëŠ” íŠ¸ë¦¬ì—ì„œ ëœë¤í•˜ê²Œ ì„ íƒë˜ëŠ” ë¶„í•  í”¼ì³ í›„ë³´ ê°œìˆ˜
```

#### 5\. SVM(Support Vector Machine)
ì´ë¡ ì€ ë³µì¡í•˜ë¯€ë¡œ pdfì°¸ê³ 
ì„ í˜• SVMê³¼ ë¹„ì„ í˜• SVM / ê²°ê³¼ì°½ì„ ë³´ë©´ ì„ í˜•ì˜ ê²½ìš° ë‹¨ìˆœ / ë¹„ì„ í˜•ì€ degree: polynomial degree, scaleì€ ë‹¤í•­ì‹ì˜ parameter scaling, CëŠ” cost   
ë§ˆì°¬ê°€ì§€ë¡œ train,test ì„¤ì •ê¹Œì§€ ë™ì¼. ì´í›„ prediction, importanceê¹Œì§€ ë™ì¼.   
ì„ í˜•ì€ ë¹„ì„ í˜•ì˜ ë¹„í•´ trainë°ì´í„°ì˜ ì •í™•ë„ëŠ” ë‚®ì§€ë§Œ, test ë°ì´í„°ì˜ ì •í™•ë„ëŠ” ë†’ìŒ. ì´ëŠ” ë¹„ì„ í˜•ë°©ì‹ì´ overfitting ê°€ëŠ¥ì„±ì´ ë†’ë‹¤ëŠ” ëœ».    
``` r
ctrl <- trainControl(method="repeatedcv",repeats = 5)  
svm_linear_fit <- train(Class ~ ., 
                         data = train, 
                         method = "svmLinear",                    # ë¹„ì„ í˜•ì˜ ê²½ìš° method = "svmPoly"
                         trControl = ctrl, 
                         preProcess = c("center","scale"),
                         metric="Accuracy")
svm_linear_fit
``` 







